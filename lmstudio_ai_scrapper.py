# Import the required libraries
import streamlit as st
from scrapegraphai.graphs import SmartScraperGraph
import requests
import json
import asyncio
import os
from pathlib import Path
from playwright.async_api import async_playwright, TimeoutError

# Set up the Streamlit app
st.title("Web Scrapping AI Agent ğŸ•µï¸â€â™‚ï¸")
st.caption("This app allows you to scrape a website using LM Studio (local model)")

# LM Studio configuration
st.sidebar.header("LM Studio é…ç½®")
lmstudio_base_url = st.sidebar.text_input(
    "LM Studio API URL", 
    value="http://192.168.2.129:1234/v1",
    help="LM Studio é»˜è®¤è¿è¡Œåœ¨ http://192.168.2.129:1234/v1"
)
model_name = st.sidebar.text_input(
    "æ¨¡å‹åç§°", 
    value="qwen/qwen3-4b-2507",
    help="åœ¨ LM Studio ä¸­åŠ è½½çš„æ¨¡å‹åç§°ï¼ˆä¾‹å¦‚ï¼šllama-3.2-3b-instructï¼‰"
)
api_key = st.sidebar.text_input(
    "API Key (å¯é€‰)", 
    value="",
    type="password",
    help="LM Studio é€šå¸¸ä¸éœ€è¦çœŸå®çš„ API keyï¼Œå¯ä»¥å¡«å†™ä»»æ„å€¼"
)

# Test LM Studio connection
if model_name and st.sidebar.button("ğŸ” æµ‹è¯•è¿æ¥", help="æµ‹è¯• LM Studio æœåŠ¡å™¨æ˜¯å¦å¯ç”¨"):
    with st.sidebar:
        with st.spinner("æ­£åœ¨æµ‹è¯•è¿æ¥..."):
            try:
                test_url = f"{lmstudio_base_url.rstrip('/v1')}/v1/models"
                response = requests.get(test_url, timeout=5)
                if response.status_code == 200:
                    models = response.json()
                    st.success("âœ… è¿æ¥æˆåŠŸï¼")
                    if 'data' in models:
                        st.info(f"å¯ç”¨æ¨¡å‹æ•°é‡: {len(models['data'])}")
                        model_names = [m.get('id', 'N/A') for m in models.get('data', [])]
                        if model_names:
                            st.write("å¯ç”¨æ¨¡å‹ï¼š")
                            for name in model_names[:5]:  # Show first 5
                                st.text(f"  â€¢ {name}")
                else:
                    st.error(f"âŒ è¿æ¥å¤±è´¥: HTTP {response.status_code}")
            except requests.exceptions.ConnectionError:
                st.error("âŒ æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨\nè¯·ç¡®ä¿ LM Studio æ­£åœ¨è¿è¡Œ")
            except requests.exceptions.Timeout:
                st.error("âŒ è¿æ¥è¶…æ—¶\nè¯·æ£€æŸ¥æœåŠ¡å™¨åœ°å€")
            except Exception as e:
                st.error(f"âŒ é”™è¯¯: {str(e)}")

# ç™»å½•/æŠ“å–è¾…åŠ©ï¼šä½¿ç”¨ Playwright è·å–éœ€è¦ç™»å½•çš„é¡µé¢ HTML
async def fetch_html_with_playwright(
    url: str,
    need_login: bool = False,
    login_url: str | None = None,
    use_storage: bool = True,
    manual_login: bool = False,
    headless: bool = True,
    page_wait_strategy: str = "domcontentloaded",
    page_timeout: int = 60,
):
    storage_state_path = "login_state.json" if need_login and use_storage else None

    async with async_playwright() as p:
        if need_login:
            st.info("ğŸ”‘ å¯åŠ¨å¸¦ç™»å½•çš„æµè§ˆå™¨...")
        browser = await p.chromium.launch(headless=headless)

        # è¯»å–å­˜å‚¨çŠ¶æ€
        context_options = {"accept_downloads": False}
        if storage_state_path and os.path.exists(storage_state_path):
            try:
                raw_state = Path(storage_state_path).read_text(encoding="utf-8").strip()
                if raw_state:
                    json.loads(raw_state)
                    context_options["storage_state"] = storage_state_path
                    st.info("ğŸ”‘ æ£€æµ‹åˆ°ä¿å­˜çš„ç™»å½•çŠ¶æ€ï¼Œå°†è‡ªåŠ¨ä½¿ç”¨")
                else:
                    st.warning("âš ï¸ æ£€æµ‹åˆ°ç©ºçš„ login_state.jsonï¼Œå·²å¿½ç•¥å¹¶åˆ é™¤ï¼Œè¯·é‡æ–°ç™»å½•")
                    os.remove(storage_state_path)
            except Exception:
                st.warning("âš ï¸ ç™»å½•çŠ¶æ€æ–‡ä»¶ä¸å¯ç”¨ï¼Œå·²å¿½ç•¥")

        context = await browser.new_context(**context_options)
        page = await context.new_page()

        try:
            if need_login:
                target_login_url = login_url if login_url else url
                st.info(f"ğŸ” æ­£åœ¨è®¿é—®ç™»å½•é¡µé¢: {target_login_url}")
                try:
                    await page.goto(
                        target_login_url,
                        wait_until=page_wait_strategy,
                        timeout=page_timeout * 1000,
                    )
                except TimeoutError:
                    st.warning("âš ï¸ ç™»å½•é¡µåŠ è½½è¶…æ—¶ï¼Œæ”¹ç”¨ domcontentloaded å†è¯•")
                    await page.goto(
                        target_login_url,
                        wait_until="domcontentloaded",
                        timeout=page_timeout * 1000,
                    )
                await page.wait_for_timeout(2000)

                if manual_login:
                    st.warning(
                        "âš ï¸ æ‰‹åŠ¨ç™»å½•æ¨¡å¼å¼€å¯ï¼Œè¯·åœ¨å¼¹å‡ºçš„æµè§ˆå™¨ä¸­å®Œæˆç™»å½•ã€‚"
                    )
                    if st.button("âœ… æˆ‘å·²ç™»å½•ï¼Œç»§ç»­"):
                        pass  # ç”±æŒ‰é’®è§¦å‘åˆ·æ–°
                    # è½®è¯¢æœ€å¤š 5 åˆ†é’Ÿ
                    waited = 0
                    interval = 3000
                    while waited < 300_000:
                        await page.wait_for_timeout(interval)
                        waited += interval
                        cur = page.url
                        st.info(f"ğŸ“ å½“å‰é¡µé¢: {cur}")
                        if "github.com" in cur and "/login" not in cur and "session" not in cur:
                            st.success("âœ… æ£€æµ‹åˆ°å·²ç™»å½•ï¼Œç»§ç»­æŠ“å–é¡µé¢")
                            break
                    else:
                        st.error("âŒ ç™»å½•è¶…æ—¶ï¼Œè¯·é‡è¯•")
                        return None

                    if storage_state_path:
                        await context.storage_state(path=storage_state_path)
                        st.success("âœ… ç™»å½•çŠ¶æ€å·²ä¿å­˜åˆ° login_state.json")

            # è®¿é—®ç›®æ ‡é¡µ
            st.info(f"ğŸŒ æ­£åœ¨è®¿é—®: {url}")
            target_wait_until = "domcontentloaded" if "github.com" in url else page_wait_strategy
            try:
                await page.goto(
                    url,
                    wait_until=target_wait_until,
                    timeout=page_timeout * 1000,
                )
            except TimeoutError:
                st.warning("âš ï¸ é¡µé¢åŠ è½½è¶…æ—¶ï¼Œæ”¹ç”¨ domcontentloaded å†è¯•")
                await page.goto(
                    url,
                    wait_until="domcontentloaded",
                    timeout=page_timeout * 1000,
                )

            await page.wait_for_timeout(2000)
            html = await page.content()
            st.success("âœ… å·²è·å–é¡µé¢ HTML")
            return html
        finally:
            await browser.close()


# Set up the configuration for the SmartScraperGraph
if model_name:
    # Advanced scraping options
    st.sidebar.subheader("é«˜çº§é€‰é¡¹")
    wait_for_load = st.sidebar.selectbox(
        "é¡µé¢åŠ è½½ç­‰å¾…ç­–ç•¥",
        ["domcontentloaded", "networkidle", "load"],
        index=1,
        help="networkidle: ç­‰å¾…æ‰€æœ‰ç½‘ç»œè¯·æ±‚å®Œæˆï¼ˆæ¨èï¼‰\ndomcontentloaded: ä»…ç­‰å¾… DOM åŠ è½½\nload: ç­‰å¾…æ‰€æœ‰èµ„æºåŠ è½½"
    )
    enable_js = st.sidebar.checkbox(
        "å¯ç”¨ JavaScript æ¸²æŸ“",
        value=True,
        help="ç¡®ä¿ JavaScript åŠ¨æ€å†…å®¹è¢«æ­£ç¡®åŠ è½½"
    )
    wait_time = st.sidebar.slider(
        "é¢å¤–ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰",
        min_value=0,
        max_value=10,
        value=3,
        help="é¡µé¢åŠ è½½åçš„é¢å¤–ç­‰å¾…æ—¶é—´ï¼Œç¡®ä¿åŠ¨æ€å†…å®¹æ¸²æŸ“å®Œæˆï¼ˆå·²åŒ…å«åœ¨è¶…æ—¶è®¾ç½®ä¸­ï¼‰"
    )
    
    # ç™»å½•é€‰é¡¹
    st.sidebar.subheader("ç™»å½•é€‰é¡¹")
    need_login = st.sidebar.checkbox("éœ€è¦ç™»å½•", value=False)
    login_url = st.sidebar.text_input(
        "ç™»å½•é¡µé¢ URL",
        value="https://github.com/login",
        help="å¦‚æœä¸ç›®æ ‡é¡µä¸åŒï¼Œè¯·å¡«å…¥ç™»å½•é¡µ"
    ) if need_login else ""
    manual_login = st.sidebar.checkbox("æ‰‹åŠ¨ç™»å½•", value=False) if need_login else False
    use_storage = st.sidebar.checkbox("ä¿å­˜ç™»å½•çŠ¶æ€", value=True) if need_login else False
    headless = st.sidebar.checkbox(
        "æ— å¤´æ¨¡å¼",
        value=not manual_login,
        help="æ‰‹åŠ¨ç™»å½•å»ºè®®å…³é—­æ— å¤´æ¨¡å¼"
    ) if need_login else True
    
    # Build loader_kwargs - only include valid browser config parameters
    loader_kwargs = {
        "load_state": wait_for_load,  # Wait for network to be idle
        "requires_js_support": enable_js,  # Enable JS rendering
        "timeout": 60 + wait_time,  # Increase timeout with additional wait time
    }
    
    # Note: Scroll parameters are not directly supported in loader_kwargs
    # The networkidle load_state should handle most dynamic content loading
    
    graph_config = {
        "llm": {
            "api_key": api_key or "lm-studio",
            "model": f"openai/{model_name}",  # Use openai/provider format for LM Studio
            "base_url": lmstudio_base_url,
            "temperature": 0,
        },
        "embeddings": {
            "model": "ollama/nomic-embed-text",
            "base_url": "http://localhost:11434",  # å¦‚æœéœ€è¦åµŒå…¥ï¼Œå¯ä»¥ä½¿ç”¨ Ollama
        },
        "verbose": True,
        "loader_kwargs": loader_kwargs,
    }
    
    # Get the URL of the website to scrape
    url = st.text_input("Enter the URL of the website you want to scrape")
    # Get the user prompt
    user_prompt = st.text_input(
        "What you want the AI agent to scrape from the website?",
        value="è¯·æå–é¡µé¢ä¸Šæ‰€æœ‰å¯è§çš„æ–‡æœ¬å†…å®¹ã€‚åŒ…æ‹¬é¡µé¢æ ‡é¢˜ã€æ®µè½æ–‡å­—ã€åˆ—è¡¨é¡¹ã€æŒ‰é’®æ–‡å­—ã€é“¾æ¥æ–‡æœ¬ç­‰æ‰€æœ‰ç”¨æˆ·å¯ä»¥çœ‹åˆ°çš„æ–‡å­—ä¿¡æ¯ã€‚è¯·å¿½ç•¥å¯¼èˆªæ å’Œé¡µè„šçš„ç‰ˆæƒä¿¡æ¯ï¼Œé‡ç‚¹å…³æ³¨é¡µé¢ä¸»ä½“åŒºåŸŸçš„å¯è§æ–‡æœ¬å†…å®¹ã€‚å¦‚æœé¡µé¢æœ‰ä¸»è¦å†…å®¹ï¼Œè¯·è¯¦ç»†åˆ—å‡ºï¼›å¦‚æœæ²¡æœ‰æ˜æ˜¾çš„ä¸»è¦å†…å®¹ï¼Œè¯·åˆ—å‡ºé¡µé¢ä¸Šæ‰€æœ‰å¯è§çš„æ–‡å­—å…ƒç´ ã€‚",
        help="ä¾‹å¦‚ï¼šæå–æ‰€æœ‰äº§å“åç§°å’Œä»·æ ¼ï¼›æå–æ–‡ç« æ ‡é¢˜å’Œå†…å®¹ï¼›æå–æ‰€æœ‰é“¾æ¥ç­‰"
    )
    
    # Debug option
    show_raw_html = st.checkbox("æ˜¾ç¤ºåŸå§‹ HTMLï¼ˆè°ƒè¯•ç”¨ï¼‰", value=False, help="æ˜¾ç¤ºæŠ“å–åˆ°çš„åŸå§‹ HTML å†…å®¹ï¼Œç”¨äºè°ƒè¯•")
    
    # Optional: JSON schema for structured output
    use_schema = st.checkbox("ä½¿ç”¨ç»“æ„åŒ–è¾“å‡º (JSON Schema)", value=False)
    json_schema = None
    if use_schema:
        schema_text = st.text_area(
            "JSON Schema (å¯é€‰)",
            value='''{
  "type": "object",
  "properties": {
    "title": {"type": "string"},
    "content": {"type": "string"},
    "links": {
      "type": "array",
      "items": {"type": "string"}
    }
  }
}''',
            help="å®šä¹‰è¾“å‡ºæ•°æ®çš„ç»“æ„"
        )
        try:
            import json
            json_schema = json.loads(schema_text)
        except:
            st.warning("JSON Schema æ ¼å¼ä¸æ­£ç¡®ï¼Œå°†ä½¿ç”¨é»˜è®¤è¾“å‡º")
    
    # Scrape the website
    if st.button("Scrape", type="primary"):
        if url and user_prompt:
            with st.spinner("æ­£åœ¨æŠ“å–ç½‘ç«™æ•°æ®..."):
                try:
                    # 1) å¦‚éœ€ç™»å½•ï¼Œå…ˆç”¨ Playwright è·å–ç™»å½•æ€é¡µé¢çš„ HTML
                    page_html = None
                    if need_login:
                        page_html = asyncio.run(
                            fetch_html_with_playwright(
                                url=url,
                                need_login=need_login,
                                login_url=login_url,
                                use_storage=use_storage,
                                manual_login=manual_login,
                                headless=headless,
                                page_wait_strategy=wait_for_load,
                                page_timeout=60 + wait_time,
                            )
                        )
                        if not page_html:
                            st.error("âŒ æœªèƒ½è·å–é¡µé¢å†…å®¹ï¼Œè¯·æ£€æŸ¥ç™»å½•çŠ¶æ€")
                            st.stop()
                    
                    # 2) æ„å»º SmartScraperGraphï¼›è‹¥æœ‰ç™»å½•é¡µ HTMLï¼Œåˆ™ä½œä¸º source ä¼ é€’
                    # ä¸ºé¿å…è¶…è¿‡æœ¬åœ°æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå¿…è¦æ—¶æˆªæ–­é¡µé¢ HTMLï¼ˆä¿å®ˆä¸€äº›ï¼‰
                    if page_html and len(page_html) > 250_000:
                        st.info("â„¹ï¸ é¡µé¢è¾ƒå¤§ï¼Œå·²è‡ªåŠ¨æˆªæ–­éƒ¨åˆ† HTML ä»¥é€‚é…æœ¬åœ°æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆçº¦ 50k å­—ç¬¦ï¼‰")
                        page_html = page_html[:250_000]
                    graph_source = page_html if page_html else url
                    smart_scraper_graph = SmartScraperGraph(
                        prompt=user_prompt,
                        source=graph_source,
                        config=graph_config,
                        schema=json_schema if json_schema else None
                    )
                    result = smart_scraper_graph.run()
                    st.success("âœ… æŠ“å–å®Œæˆï¼")
                    
                    # Display results in a better format
                    st.subheader("ğŸ“Š æŠ“å–ç»“æœ")
                    
                    # Show raw HTML if debug mode is enabled
                    if show_raw_html and page_html:
                        with st.expander("ğŸ” ç™»å½•åé¡µé¢ HTMLï¼ˆè°ƒè¯•ï¼‰", expanded=False):
                            st.code(page_html[:5000] + "\n... (æˆªæ–­)", language='html')
                    elif show_raw_html:
                        with st.expander("ğŸ” åŸå§‹ HTML å†…å®¹ï¼ˆè°ƒè¯•ï¼‰", expanded=False):
                            try:
                                if hasattr(smart_scraper_graph, 'final_state') and smart_scraper_graph.final_state:
                                    html_content = smart_scraper_graph.final_state.get('chunks', [])
                                    if html_content:
                                        st.code(html_content[0] if isinstance(html_content, list) else str(html_content), language='html')
                                    else:
                                        st.info("æ— æ³•è·å–åŸå§‹ HTMLï¼Œè¯·æ£€æŸ¥æŠ“å–è¿‡ç¨‹")
                                else:
                                    st.info("æ— æ³•è·å–åŸå§‹ HTMLï¼Œè¯·æ£€æŸ¥æŠ“å–è¿‡ç¨‹")
                            except Exception as e:
                                st.warning(f"æ— æ³•æ˜¾ç¤ºåŸå§‹ HTML: {str(e)}")
                    
                    # If result is a dict with 'content' key, extract it
                    if isinstance(result, dict):
                        if 'content' in result:
                            st.markdown("### å†…å®¹ï¼š")
                            st.markdown(result['content'])
                            
                            # Show full result in expander
                            with st.expander("æŸ¥çœ‹å®Œæ•´ç»“æœ (JSON)"):
                                import json
                                st.json(result)
                        else:
                            # Show all keys
                            for key, value in result.items():
                                st.markdown(f"### {key}ï¼š")
                                if isinstance(value, (dict, list)):
                                    st.json(value)
                                else:
                                    st.write(value)
                    else:
                        # If result is a string or other type
                        st.markdown("### å†…å®¹ï¼š")
                        st.write(result)
                        
                        # Show raw result
                        with st.expander("æŸ¥çœ‹åŸå§‹ç»“æœ"):
                            st.write(result)
                            
                except Exception as e:
                    error_msg = str(e)
                    st.error(f"âŒ é”™è¯¯: {error_msg}")
                    
                    # Provide specific guidance based on error type
                    if "503" in error_msg or "InternalServerError" in error_msg:
                        st.warning("""
                        **503 é”™è¯¯ - LM Studio æœåŠ¡å™¨ä¸å¯ç”¨**
                        
                        å¯èƒ½çš„åŸå› ï¼š
                        1. ğŸ”´ LM Studio æœåŠ¡å™¨æœªå¯åŠ¨
                        2. ğŸ”´ æ¨¡å‹æœªåŠ è½½æˆ–åŠ è½½å¤±è´¥
                        3. ğŸ”´ æœåŠ¡å™¨åœ°å€ä¸æ­£ç¡®
                        4. ğŸ”´ æœåŠ¡å™¨è¿‡è½½æˆ–å´©æºƒ
                        
                        **è§£å†³æ–¹æ³•ï¼š**
                        1. æ‰“å¼€ LM Studio åº”ç”¨
                        2. ç¡®ä¿æ¨¡å‹å·²åŠ è½½ï¼ˆåœ¨ "Chat" æ ‡ç­¾ä¸­å¯ä»¥çœ‹åˆ°æ¨¡å‹ï¼‰
                        3. åˆ‡æ¢åˆ° "Server" æ ‡ç­¾ï¼Œç‚¹å‡» "Start Server"
                        4. ç¡®è®¤æœåŠ¡å™¨åœ°å€æ˜¯ `http://localhost:1234/v1`
                        5. ç‚¹å‡»ä¾§è¾¹æ çš„ "ğŸ” æµ‹è¯•è¿æ¥" æŒ‰é’®éªŒè¯è¿æ¥
                        """)
                    elif "ConnectionError" in error_msg or "æ— æ³•è¿æ¥" in error_msg:
                        st.warning("""
                        **è¿æ¥é”™è¯¯**
                        
                        è¯·æ£€æŸ¥ï¼š
                        1. LM Studio æ˜¯å¦æ­£åœ¨è¿è¡Œ
                        2. æœåŠ¡å™¨åœ°å€æ˜¯å¦æ­£ç¡®ï¼ˆé»˜è®¤ï¼šhttp://localhost:1234/v1ï¼‰
                        3. é˜²ç«å¢™æ˜¯å¦é˜»æ­¢äº†è¿æ¥
                        """)
                    else:
                        st.info("ğŸ’¡ è¯·ç¡®ä¿ï¼š\n1. LM Studio æ­£åœ¨è¿è¡Œ\n2. æ¨¡å‹å·²åŠ è½½\n3. æœåŠ¡å™¨åœ°å€æ­£ç¡®\n4. æ¨¡å‹åç§°æ­£ç¡®\n5. ç½‘ç«™å¯ä»¥æ­£å¸¸è®¿é—®")
                    
                    with st.expander("æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯"):
                        import traceback
                        st.code(traceback.format_exc())
        else:
            st.warning("âš ï¸ è¯·å¡«å†™ç½‘ç«™ URL å’ŒæŠ“å–æç¤º")
else:
    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§è¾¹æ é…ç½® LM Studio è®¾ç½®")
    st.markdown("""
    ### ä½¿ç”¨è¯´æ˜ï¼š
    1. **å¯åŠ¨ LM Studio** å¹¶åŠ è½½ä¸€ä¸ªæ¨¡å‹
    2. **ç¡®ä¿æœåŠ¡å™¨æ­£åœ¨è¿è¡Œ**ï¼ˆLM Studio ç•Œé¢ä¸­çš„ "Server" æ ‡ç­¾ï¼‰
    3. **è¾“å…¥æ¨¡å‹åç§°**ï¼ˆåœ¨ LM Studio ä¸­æ˜¾ç¤ºçš„æ¨¡å‹åç§°ï¼‰
    4. **é…ç½® API URL**ï¼ˆé»˜è®¤ï¼šhttp://localhost:1234/v1ï¼‰
    5. å¡«å†™ç½‘ç«™ URL å’ŒæŠ“å–æç¤ºï¼Œç„¶åç‚¹å‡» "Scrape"
    """)

