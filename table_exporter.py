"""
è¡¨æ ¼å¯¼å‡ºæŠ“å–å·¥å…·
è‡ªåŠ¨ç‚¹å‡»ç½‘é¡µä¸Šçš„å¯¼å‡ºæŒ‰é’®ï¼Œä¸‹è½½å¹¶è§£æè¡¨æ ¼æ•°æ®
"""

import streamlit as st
import asyncio
from playwright.async_api import async_playwright, TimeoutError
import pandas as pd
import os
import tempfile
from pathlib import Path
import json
from bs4 import BeautifulSoup

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(page_title="è¡¨æ ¼å¯¼å‡ºæŠ“å–å·¥å…·", layout="wide")

st.title("è‡ªåŠ¨æŠ“å–å·¥å…·")
st.caption("è‡ªåŠ¨ç‚¹å‡»ç½‘é¡µä¸Šçš„å¯¼å‡ºæŒ‰é’®ï¼Œä¸‹è½½å¹¶è§£æè¡¨æ ¼æ•°æ®")

# é…ç½®é€‰é¡¹
st.sidebar.header("é…ç½®é€‰é¡¹")

url = st.text_input("ç½‘é¡µ URL", placeholder="https://example.com")

# å…ˆåˆå§‹åŒ–ï¼Œé¿å…æœªå®šä¹‰
button_description = ""
data_description = ""

# æŠ“å–æ¨¡å¼é€‰æ‹©
scrape_mode = st.radio(
    "æŠ“å–æ¨¡å¼",
    ["ç‚¹å‡»å¯¼å‡ºæŒ‰é’®", "æŠ“å–é¡µé¢æ•°æ®"],
    help="é€‰æ‹©æ˜¯ç‚¹å‡»å¯¼å‡ºæŒ‰é’®ä¸‹è½½æ–‡ä»¶ï¼Œè¿˜æ˜¯ç›´æ¥ä»é¡µé¢æŠ“å–æ•°æ®"
)

if scrape_mode == "ç‚¹å‡»å¯¼å‡ºæŒ‰é’®":
    button_description = st.text_input(
        "å¯¼å‡ºæŒ‰é’®æè¿°",
        placeholder="ä¾‹å¦‚ï¼šå¯¼å‡ºè¡¨æ ¼ã€Exportã€ä¸‹è½½ CSV ç­‰",
        help="æè¿°å¯¼å‡ºæŒ‰é’®çš„æ–‡å­—æˆ–ç‰¹å¾ï¼ŒAI ä¼šè‡ªåŠ¨æ‰¾åˆ°å¹¶ç‚¹å‡»"
    )
else:
    data_description = st.text_input(
        "æ•°æ®æè¿°",
        placeholder="ä¾‹å¦‚ï¼šæ‰€æœ‰ä»“åº“ã€äº§å“åˆ—è¡¨ã€è¡¨æ ¼æ•°æ®ç­‰",
        help="æè¿°è¦æŠ“å–çš„æ•°æ®å†…å®¹ï¼Œå·¥å…·ä¼šä»é¡µé¢ä¸­æå–"
    )

# ç™»å½•é€‰é¡¹
st.sidebar.subheader("ğŸ” ç™»å½•é€‰é¡¹")
need_login = st.sidebar.checkbox("éœ€è¦ç™»å½•", value=False, help="å¦‚æœç½‘é¡µéœ€è¦ç™»å½•æ‰èƒ½è®¿é—®ï¼Œè¯·å‹¾é€‰æ­¤é¡¹")
login_url = None
if need_login:
    login_url = st.sidebar.text_input(
        "ç™»å½•é¡µé¢ URL",
        placeholder="https://example.com/login",
        help="ç™»å½•é¡µé¢çš„ URLï¼ˆå¦‚æœä¸ç›®æ ‡é¡µé¢ä¸åŒï¼‰"
    )
    use_storage = st.sidebar.checkbox("ä¿å­˜ç™»å½•çŠ¶æ€", value=True, help="ä¿å­˜ç™»å½•çŠ¶æ€ï¼Œä¸‹æ¬¡ä½¿ç”¨æ—¶è‡ªåŠ¨ç™»å½•")
    manual_login = st.sidebar.checkbox("æ‰‹åŠ¨ç™»å½•", value=False, help="åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨ç™»å½•ï¼ˆæ¨èï¼‰")

# é«˜çº§é€‰é¡¹
st.sidebar.subheader("é«˜çº§é€‰é¡¹")
wait_time = st.sidebar.slider("ç‚¹å‡»åç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰", 1, 30, 5)
download_timeout = st.sidebar.slider("ä¸‹è½½è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰", 10, 120, 60)
headless = st.sidebar.checkbox("æ— å¤´æ¨¡å¼", value=False if need_login and manual_login else True, help="å¦‚æœæ‰‹åŠ¨ç™»å½•ï¼Œå»ºè®®å…³é—­æ— å¤´æ¨¡å¼")
page_wait_strategy = st.sidebar.selectbox(
    "é¡µé¢åŠ è½½ç­‰å¾…ç­–ç•¥",
    ["networkidle", "load", "domcontentloaded"],
    index=0,
    help="networkidleï¼šç­‰å¾…ç½‘ç»œç©ºé—²ï¼›loadï¼šç­‰å¾…æ‰€æœ‰èµ„æºåŠ è½½ï¼›domcontentloadedï¼šä»…ç­‰å¾… DOM"
)
page_timeout = st.sidebar.slider("é¡µé¢åŠ è½½è¶…æ—¶ï¼ˆç§’ï¼‰", 30, 180, 60)

# æ–‡ä»¶ç±»å‹é€‰æ‹©
file_types = st.sidebar.multiselect(
    "æ”¯æŒçš„æ–‡ä»¶ç±»å‹",
    ["CSV", "Excel (.xlsx)", "JSON", "TSV"],
    default=["CSV", "Excel (.xlsx)"]
)

async def find_and_click_button(page, description):
    """ä½¿ç”¨ AI æˆ–æ–‡æœ¬åŒ¹é…æ‰¾åˆ°å¹¶ç‚¹å‡»æŒ‰é’®"""
    try:
        # æ–¹æ³•1: é€šè¿‡æ–‡æœ¬å†…å®¹æŸ¥æ‰¾
        button_texts = [
            description,
            "å¯¼å‡º",
            "Export",
            "ä¸‹è½½",
            "Download",
            "å¯¼å‡ºè¡¨æ ¼",
            "Export Table",
            "å¯¼å‡º CSV",
            "Export CSV",
            "å¯¼å‡º Excel",
            "Export Excel"
        ]
        
        for text in button_texts:
            try:
                # å°è¯•é€šè¿‡æ–‡æœ¬æŸ¥æ‰¾æŒ‰é’®
                button = await page.query_selector(f'button:has-text("{text}")')
                if button:
                    await button.click()
                    st.success(f"âœ… æ‰¾åˆ°å¹¶ç‚¹å‡»äº†æŒ‰é’®: {text}")
                    return True
                
                # å°è¯•é€šè¿‡é“¾æ¥æŸ¥æ‰¾
                link = await page.query_selector(f'a:has-text("{text}")')
                if link:
                    await link.click()
                    st.success(f"âœ… æ‰¾åˆ°å¹¶ç‚¹å‡»äº†é“¾æ¥: {text}")
                    return True
            except:
                continue
        
        # æ–¹æ³•2: é€šè¿‡å±æ€§æŸ¥æ‰¾ï¼ˆdata-*, class, idï¼‰
        selectors = [
            f'button[data-action*="export" i]',
            f'button[class*="export" i]',
            f'button[id*="export" i]',
            f'a[data-action*="export" i]',
            f'a[class*="export" i]',
            f'a[id*="export" i]',
            f'*[aria-label*="export" i]',
            f'*[title*="export" i]',
        ]
        
        for selector in selectors:
            try:
                element = await page.query_selector(selector)
                if element:
                    await element.click()
                    st.success(f"âœ… é€šè¿‡é€‰æ‹©å™¨æ‰¾åˆ°å¹¶ç‚¹å‡»äº†æŒ‰é’®: {selector}")
                    return True
            except:
                continue
        
        st.warning("âš ï¸ æœªæ‰¾åˆ°å¯¼å‡ºæŒ‰é’®ï¼Œè¯·æ£€æŸ¥æŒ‰é’®æè¿°æ˜¯å¦æ­£ç¡®")
        return False
        
    except Exception as e:
        st.error(f"âŒ æŸ¥æ‰¾æŒ‰é’®æ—¶å‡ºé”™: {str(e)}")
        return False

async def download_file(page, download_dir, timeout=60):
    """ç­‰å¾…æ–‡ä»¶ä¸‹è½½"""
    try:
        # ç­‰å¾…ä¸‹è½½äº‹ä»¶
        async with page.expect_download(timeout=timeout * 1000) as download_info:
            pass
        
        download = await download_info.value
        file_path = os.path.join(download_dir, download.suggested_filename)
        await download.save_as(file_path)
        
        st.success(f"âœ… æ–‡ä»¶ä¸‹è½½æˆåŠŸ: {download.suggested_filename}")
        return file_path
        
    except Exception as e:
        st.error(f"âŒ ä¸‹è½½æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        return None

def parse_table_file(file_path):
    """è§£æè¡¨æ ¼æ–‡ä»¶"""
    file_ext = Path(file_path).suffix.lower()
    
    try:
        if file_ext == '.csv':
            df = pd.read_csv(file_path, encoding='utf-8')
            return df, "CSV"
        elif file_ext in ['.xlsx', '.xls']:
            df = pd.read_excel(file_path)
            return df, "Excel"
        elif file_ext == '.json':
            df = pd.read_json(file_path)
            return df, "JSON"
        elif file_ext == '.tsv':
            df = pd.read_csv(file_path, sep='\t', encoding='utf-8')
            return df, "TSV"
        else:
            st.error(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}")
            return None, None
    except Exception as e:
        st.error(f"âŒ è§£ææ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        return None, None

async def scrape_page_data(page, description, page_wait_strategy="networkidle", page_timeout=60, extra_wait=2):
    """ä»é¡µé¢ä¸­æŠ“å–è¡¨æ ¼/åˆ—è¡¨æ•°æ®"""
    try:
        st.info("ğŸ“Š æ­£åœ¨ä»é¡µé¢æå–æ•°æ®...")
        
        # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½ï¼Œä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„ç­–ç•¥
        try:
            await page.wait_for_load_state(page_wait_strategy, timeout=page_timeout * 1000)
        except TimeoutError:
            st.warning("âš ï¸ é¡µé¢åŠ è½½ç­‰å¾…è¶…æ—¶ï¼Œæ”¹ç”¨ domcontentloaded å†è¯•")
            await page.wait_for_load_state("domcontentloaded", timeout=page_timeout * 1000)
        await page.wait_for_timeout(extra_wait * 1000)
        
        # å°è¯•æŸ¥æ‰¾è¡¨æ ¼
        tables = await page.query_selector_all('table')
        if tables:
            st.info(f"âœ… æ‰¾åˆ° {len(tables)} ä¸ªè¡¨æ ¼")
            # è¿”å›ç¬¬ä¸€ä¸ªè¡¨æ ¼çš„ HTML
            table_html = await tables[0].inner_html()
            return table_html, "table"

        # GitHub ä»“åº“åˆ—è¡¨ä¼˜å…ˆå¤„ç†ï¼Œé¿å…è¢«é€šç”¨åˆ—è¡¨åŒ¹é…æŠ¢èµ°
        if "github.com" in page.url and "repositories" in page.url:
            # ä» URL æå–ç”¨æˆ·åï¼ˆç”¨äºè¿‡æ»¤é“¾æ¥ï¼‰
            try:
                from urllib.parse import urlparse
                path_parts = urlparse(page.url).path.strip("/").split("/")
                gh_user = path_parts[0] if path_parts else ""
            except Exception:
                gh_user = ""

            # ä¼˜å…ˆç”¨ JS ç›´æ¥æå–ç»“æ„åŒ–æ•°æ®ï¼Œé¿å… HTML ç»“æ„å˜åŒ–
            if gh_user:
                try:
                    # ç­‰å¾…ä»“åº“åˆ—è¡¨å…ƒç´ å‡ºç°ï¼Œæœ€å¤š 15 ç§’
                    await page.wait_for_selector(
                        '[data-testid="repository-list"] li, [data-testid="results-list"] li, article',
                        timeout=15000
                    )
                except Exception:
                    pass

                repos = await page.evaluate(
                    """(user) => {
                        const cards = Array.from(document.querySelectorAll(
                            '[data-testid="repository-list"] li, [data-testid="results-list"] li, article, li'
                        ));
                        const data = [];
                        for (const el of cards) {
                            const link = el.querySelector(`a[href*="/${user}/"]`);
                            if (!link) continue;
                            const name = link.textContent.trim();
                            if (!name) continue;
                            const href = link.getAttribute('href') || '';
                            const descEl = el.querySelector('p, .repo-description, [itemprop="description"]');
                            const langEl = el.querySelector('[itemprop="programmingLanguage"], .repo-language-color + span, [data-testid="repo-card-language"]');
                            const starEl = el.querySelector('a[href$="/stargazers"], [data-testid="stargazers"]');
                            data.push({
                                "ä»“åº“åç§°": name,
                                "é“¾æ¥": href.startsWith('http') ? href : `https://github.com${href}`,
                                "æè¿°": descEl ? descEl.textContent.trim() : "",
                                "è¯­è¨€": langEl ? langEl.textContent.trim() : "",
                                "æ˜Ÿæ ‡æ•°": starEl ? starEl.textContent.trim() : ""
                            });
                        }
                        return data.filter(item => item["ä»“åº“åç§°"]);
                    }""",
                    gh_user,
                )
                if repos:
                    st.info(f"âœ… ç›´æ¥æå–åˆ° {len(repos)} ä¸ªä»“åº“")
                    return repos, "github_repos_json"

            # å…œåº•ï¼šHTML æå–
            repo_list = await page.query_selector('[data-testid="repository-list"]')
            if not repo_list:
                repo_list = await page.query_selector('.repo-list')
            if not repo_list:
                repo_list = await page.query_selector('[itemtype="http://schema.org/CodeRepository"]')
            if repo_list:
                st.info("âœ… æ‰¾åˆ° GitHub ä»“åº“åˆ—è¡¨")
                repo_html = await repo_list.inner_html()
                return repo_html, "github_repos"

        # å°è¯•æŸ¥æ‰¾åˆ—è¡¨
        lists = await page.query_selector_all('ul, ol')
        if lists:
            st.info(f"âœ… æ‰¾åˆ° {len(lists)} ä¸ªåˆ—è¡¨")
            # æŸ¥æ‰¾åŒ…å«æè¿°å…³é”®è¯çš„åˆ—è¡¨
            for list_elem in lists:
                list_text = await list_elem.inner_text()
                if description.lower() in list_text.lower() or len(list_text) > 50:
                    list_html = await list_elem.inner_html()
                    return list_html, "list"
        
        # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œå°è¯•è·å–æ•´ä¸ªé¡µé¢å†…å®¹
        st.warning("âš ï¸ æœªæ‰¾åˆ°ç‰¹å®šæ•°æ®å®¹å™¨ï¼Œå°è¯•æå–æ•´ä¸ªé¡µé¢å†…å®¹")
        body = await page.query_selector('body')
        if body:
            body_html = await body.inner_html()
            return body_html, "full_page"
        
        return None, None
        
    except Exception as e:
        st.error(f"âŒ æŠ“å–é¡µé¢æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        return None, None

def parse_html_to_dataframe(html_content, data_type, url=""):
    """å°† HTML å†…å®¹è§£æä¸º DataFrame"""
    try:
        # ç›´æ¥æ”¯æŒç»“æ„åŒ–åˆ—è¡¨ï¼ˆä¾‹å¦‚ GitHub JS æŠ½å–ï¼‰
        if data_type == "github_repos_json" and isinstance(html_content, list):
            if html_content:
                return pd.DataFrame(html_content)
            return None

        soup = BeautifulSoup(html_content, 'html.parser')
        
        if data_type == "table":
            # è§£æè¡¨æ ¼
            table = soup.find('table')
            if table:
                df = pd.read_html(str(table))[0]
                return df
        
        elif data_type == "github_repos":
            # è§£æ GitHub ä»“åº“åˆ—è¡¨
            repos = []
            repo_items = soup.find_all(['article', 'li'], class_=lambda x: x and ('repo' in x.lower() or 'repository' in x.lower()))
            
            if not repo_items:
                # å°è¯•å…¶ä»–é€‰æ‹©å™¨
                repo_items = soup.find_all('div', class_=lambda x: x and 'repo' in x.lower())
            
            for item in repo_items:
                repo_data = {}
                
                # æå–ä»“åº“åç§°
                name_elem = item.find('a', href=lambda x: x and '/Jehuge/' in x)
                if name_elem:
                    repo_data['ä»“åº“åç§°'] = name_elem.get_text(strip=True)
                    repo_data['é“¾æ¥'] = 'https://github.com' + name_elem.get('href', '')
                
                # æå–æè¿°
                desc_elem = item.find('p', class_=lambda x: x and 'description' in x.lower())
                if not desc_elem:
                    desc_elem = item.find('p')
                if desc_elem:
                    repo_data['æè¿°'] = desc_elem.get_text(strip=True)
                
                # æå–è¯­è¨€
                lang_elem = item.find('span', itemprop='programmingLanguage')
                if lang_elem:
                    repo_data['è¯­è¨€'] = lang_elem.get_text(strip=True)
                
                # æå–æ˜Ÿæ ‡æ•°
                star_elem = item.find('a', href=lambda x: x and 'stargazers' in x)
                if star_elem:
                    repo_data['æ˜Ÿæ ‡æ•°'] = star_elem.get_text(strip=True)
                
                if repo_data:
                    repos.append(repo_data)
            
            if repos:
                df = pd.DataFrame(repos)
                return df
        
        elif data_type == "list":
            # è§£æåˆ—è¡¨
            items = soup.find_all('li')
            data = []
            for item in items:
                text = item.get_text(strip=True)
                if text:
                    data.append({'å†…å®¹': text})
            if data:
                df = pd.DataFrame(data)
                return df
        
        # å¦‚æœä»¥ä¸Šéƒ½ä¸åŒ¹é…ï¼Œå°è¯•æå–æ‰€æœ‰é“¾æ¥å’Œæ–‡æœ¬
        st.info("ğŸ“ å°è¯•æå–é¡µé¢ä¸­çš„é“¾æ¥å’Œæ–‡æœ¬...")
        links = soup.find_all('a', href=True)
        data = []
        for link in links[:100]:  # é™åˆ¶æ•°é‡
            text = link.get_text(strip=True)
            href = link.get('href', '')
            if text and len(text) < 200:  # è¿‡æ»¤å¤ªé•¿çš„æ–‡æœ¬
                data.append({
                    'æ–‡æœ¬': text,
                    'é“¾æ¥': href if href.startswith('http') else url + href
                })
        
        if data:
            df = pd.DataFrame(data)
            return df
        
        return None
        
    except Exception as e:
        st.error(f"âŒ è§£æ HTML æ—¶å‡ºé”™: {str(e)}")
        import traceback
        st.code(traceback.format_exc())
        return None

async def scrape_table(url, description, wait_time, download_timeout, headless, 
                       need_login=False, login_url=None, use_storage=False, manual_login=False,
                       scrape_mode="ç‚¹å‡»å¯¼å‡ºæŒ‰é’®",
                       page_wait_strategy="networkidle",
                       page_timeout=60):
    """ä¸»æŠ“å–å‡½æ•°"""
    with tempfile.TemporaryDirectory() as download_dir:
        async with async_playwright() as p:
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¿å­˜çš„ç™»å½•çŠ¶æ€
            storage_state_path = None
            if need_login and use_storage:
                storage_state_path = "login_state.json"
                if os.path.exists(storage_state_path):
                    try:
                        # å…ˆéªŒè¯ JSON æ˜¯å¦æœ‰æ•ˆï¼Œé¿å…ç©ºæ–‡ä»¶/æŸåæ–‡ä»¶å¯¼è‡´ JSONDecodeError
                        raw_state = Path(storage_state_path).read_text(encoding="utf-8").strip()
                        if raw_state:
                            json.loads(raw_state)
                            st.info("ğŸ”‘ æ£€æµ‹åˆ°ä¿å­˜çš„ç™»å½•çŠ¶æ€ï¼Œå°†è‡ªåŠ¨ä½¿ç”¨")
                        else:
                            st.warning("âš ï¸ æ£€æµ‹åˆ°ç©ºçš„ login_state.jsonï¼Œå·²å¿½ç•¥å¹¶åˆ é™¤ï¼Œè¯·é‡æ–°ç™»å½•")
                            try:
                                os.remove(storage_state_path)
                            except OSError:
                                pass
                            storage_state_path = None
                    except json.JSONDecodeError:
                        st.warning("âš ï¸ ä¿å­˜çš„ç™»å½•çŠ¶æ€æ–‡ä»¶å·²æŸåï¼Œå·²å¿½ç•¥å¹¶åˆ é™¤ï¼Œè¯·é‡æ–°ç™»å½•")
                        try:
                            os.remove(storage_state_path)
                        except OSError:
                            pass
                        storage_state_path = None
                    except Exception as e:
                        st.warning(f"âš ï¸ è¯»å–ç™»å½•çŠ¶æ€æ—¶å‡ºé”™ï¼Œå·²å¿½ç•¥: {e}")
                        storage_state_path = None
            
            # å¯åŠ¨æµè§ˆå™¨
            browser = await p.chromium.launch(headless=headless)
            
            # åˆ›å»ºä¸Šä¸‹æ–‡ï¼Œå¦‚æœæœ‰ä¿å­˜çš„çŠ¶æ€åˆ™åŠ è½½
            context_options = {
                "accept_downloads": True,
            }
            if storage_state_path and os.path.exists(storage_state_path):
                context_options["storage_state"] = storage_state_path
            
            context = await browser.new_context(**context_options)
            page = await context.new_page()
            
            try:
                # å¦‚æœéœ€è¦ç™»å½•
                if need_login:
                    login_target_url = login_url if login_url else url
                    st.info(f"ğŸ” æ­£åœ¨è®¿é—®ç™»å½•é¡µé¢: {login_target_url}")
                    login_wait_until = "domcontentloaded" if "github.com" in login_target_url else page_wait_strategy
                    try:
                        # ä½¿ç”¨ä¸é¡µé¢æŠ“å–ç›¸åŒçš„ç­‰å¾…ç­–ç•¥ä¸è¶…æ—¶ï¼Œé¿å… networkidle å¡æ­»ï¼ˆå¦‚ GitHub é•¿è½®è¯¢ï¼‰
                        await page.goto(
                            login_target_url,
                            wait_until=login_wait_until,
                            timeout=page_timeout * 1000
                        )
                    except TimeoutError:
                        st.warning("âš ï¸ ç™»å½•é¡µåŠ è½½ç­‰å¾…è¶…æ—¶ï¼Œå°è¯•ä½¿ç”¨ domcontentloaded å†è¯•")
                        await page.goto(
                            login_target_url,
                            wait_until="domcontentloaded",
                            timeout=page_timeout * 1000
                        )
                    await page.wait_for_timeout(2000)
                    
                    if manual_login:
                        # æ‰‹åŠ¨ç™»å½•æ¨¡å¼
                        st.warning("""
                        âš ï¸ **æ‰‹åŠ¨ç™»å½•æ¨¡å¼å·²å¯ç”¨**
                        
                        è¯·åœ¨æµè§ˆå™¨çª—å£ä¸­ï¼š
                        1. è¾“å…¥ç”¨æˆ·åå’Œå¯†ç 
                        2. å®Œæˆç™»å½•æµç¨‹ï¼ˆåŒ…æ‹¬éªŒè¯ç ã€ä¸¤æ­¥éªŒè¯ç­‰ï¼‰
                        3. ç¡®ä¿å·²æˆåŠŸç™»å½•åˆ°ç›®æ ‡é¡µé¢
                        4. ç™»å½•æˆåŠŸåï¼Œå·¥å…·ä¼šè‡ªåŠ¨æ£€æµ‹å¹¶ç»§ç»­
                        """)
                        
                        # ç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨ç™»å½•
                        st.info("â³ ç­‰å¾…æ‚¨å®Œæˆç™»å½•...")
                        
                        # åˆå§‹åŒ–ç™»å½•ç¡®è®¤çŠ¶æ€
                        if 'login_confirmed' not in st.session_state:
                            st.session_state.login_confirmed = False
                        
                        # åˆ›å»ºçŠ¶æ€æ˜¾ç¤ºå ä½ç¬¦
                        status_placeholder = st.empty()
                        # åˆ›å»ºâ€œæˆ‘å·²ç™»å½•ï¼Œç»§ç»­â€æŒ‰é’®ï¼ˆå…è®¸æ‰‹åŠ¨è·³è¿‡æ£€æµ‹ï¼‰
                        if st.button("âœ… æˆ‘å·²ç™»å½•ï¼Œç»§ç»­", type="secondary"):
                            st.session_state.login_confirmed = True
                        
                        # è½®è¯¢æ£€æŸ¥ç™»å½•çŠ¶æ€ï¼ˆæ¯3ç§’æ£€æŸ¥ä¸€æ¬¡ï¼‰
                        max_wait_time = 300  # æœ€å¤šç­‰å¾…5åˆ†é’Ÿ
                        wait_interval = 3000  # æ¯3ç§’æ£€æŸ¥ä¸€æ¬¡
                        waited_time = 0
                        login_confirmed = False
                        
                        while waited_time < max_wait_time * 1000 and not login_confirmed:
                            await page.wait_for_timeout(wait_interval)
                            waited_time += wait_interval
                            
                            # æ£€æŸ¥å½“å‰ URL
                            current_url = page.url
                            # GitHub ç™»å½•æˆåŠŸåå¸¸è§çš„å·²ç™»å½•å…ƒç´ 
                            github_authed = False
                            if "github.com" in current_url:
                                try:
                                    # ç”¨æˆ·å¤´åƒèœå•æˆ– profile é“¾æ¥
                                    if await page.query_selector('[data-testid="user-profile-link"], summary[aria-label*="profile" i], summary[aria-label*="View profile" i]'):
                                        github_authed = True
                                except Exception:
                                    pass
                            
                            
                            # æ›´æ–°çŠ¶æ€æ˜¾ç¤ºï¼ˆä¸åŒ…å«æŒ‰é’®ï¼Œé¿å…é‡å¤åˆ›å»ºï¼‰
                            with status_placeholder.container():
                                st.info(f"ğŸ“ å½“å‰é¡µé¢: {current_url}")
                                st.info(f"â±ï¸ å·²ç­‰å¾…: {waited_time // 1000} ç§’")
                                
                                # å¯¹äº GitHubï¼Œè‡ªåŠ¨æ£€æµ‹æ˜¯å¦å·²ç™»å½•
                                if "github.com" in current_url:
                                    # GitHub ç™»å½•æˆåŠŸåä¼šè·³è½¬åˆ°ä¸»é¡µæˆ–ç”¨æˆ·é¡µé¢ï¼Œæˆ–æ£€æµ‹åˆ°å¤´åƒ
                                    if github_authed or ("/login" not in current_url and "session" not in current_url):
                                        st.success("âœ… æ£€æµ‹åˆ°å·²ç™»å½• GitHubï¼Œè‡ªåŠ¨ç»§ç»­...")
                                        login_confirmed = True
                                        break
                                else:
                                    # å…¶ä»–ç½‘ç«™ï¼Œæ£€æŸ¥æ˜¯å¦è¿˜åœ¨ç™»å½•é¡µé¢
                                    if "/login" not in current_url.lower() and "signin" not in current_url.lower():
                                        st.success("âœ… æ£€æµ‹åˆ°å·²ç¦»å¼€ç™»å½•é¡µé¢ï¼Œè‡ªåŠ¨ç»§ç»­...")
                                        login_confirmed = True
                                        break
                        
                        # æ¸…é™¤çŠ¶æ€æ˜¾ç¤º
                        status_placeholder.empty()
                        
                        if not login_confirmed:
                            st.error("âŒ ç™»å½•è¶…æ—¶ï¼Œè¯·é‡è¯•")
                            return None
                        
                        # ä¿å­˜ç™»å½•çŠ¶æ€
                        if use_storage:
                            await context.storage_state(path=storage_state_path)
                            st.success("âœ… ç™»å½•çŠ¶æ€å·²ä¿å­˜åˆ° login_state.json")
                    else:
                        # è‡ªåŠ¨ç™»å½•ï¼ˆéœ€è¦ç”¨æˆ·æä¾›ç™»å½•ä¿¡æ¯ï¼‰
                        st.info("ğŸ’¡ æç¤ºï¼šå¦‚æœè‡ªåŠ¨ç™»å½•å¤±è´¥ï¼Œè¯·ä½¿ç”¨æ‰‹åŠ¨ç™»å½•æ¨¡å¼")
                        # è¿™é‡Œå¯ä»¥æ·»åŠ è‡ªåŠ¨å¡«å†™è¡¨å•çš„é€»è¾‘
                        # ä½†ä¸ºäº†å®‰å…¨ï¼Œå»ºè®®ä½¿ç”¨æ‰‹åŠ¨ç™»å½•
                
                # è®¿é—®ç›®æ ‡ç½‘é¡µ
                st.info(f"ğŸŒ æ­£åœ¨è®¿é—®: {url}")
                target_wait_until = "domcontentloaded" if "github.com" in url else page_wait_strategy
                try:
                    await page.goto(url, wait_until=target_wait_until, timeout=page_timeout * 1000)
                except TimeoutError:
                    st.warning("âš ï¸ é¡µé¢åŠ è½½è¶…æ—¶ï¼Œå°è¯•ä½¿ç”¨ domcontentloaded å†è¯•ä¸€æ¬¡")
                    await page.goto(url, wait_until="domcontentloaded", timeout=page_timeout * 1000)
                await page.wait_for_timeout(wait_time * 1000)  # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½/åŠ¨æ€æ¸²æŸ“
                
                # æ ¹æ®æ¨¡å¼é€‰æ‹©ä¸åŒçš„å¤„ç†æ–¹å¼
                if scrape_mode == "ç‚¹å‡»å¯¼å‡ºæŒ‰é’®":
                    # æŸ¥æ‰¾å¹¶ç‚¹å‡»æŒ‰é’®
                    st.info("ğŸ” æ­£åœ¨æŸ¥æ‰¾å¯¼å‡ºæŒ‰é’®...")
                    clicked = await find_and_click_button(page, button_description)
                    
                    if not clicked:
                        return None
                    
                    # ç­‰å¾…æ–‡ä»¶ä¸‹è½½
                    st.info(f"â³ ç­‰å¾…æ–‡ä»¶ä¸‹è½½ï¼ˆæœ€å¤š {download_timeout} ç§’ï¼‰...")
                    await page.wait_for_timeout(wait_time * 1000)  # ç­‰å¾…æŒ‰é’®å“åº”
                    
                    file_path = await download_file(page, download_dir, download_timeout)
                    
                    if file_path:
                        # è§£ææ–‡ä»¶
                        st.info("ğŸ“Š æ­£åœ¨è§£æè¡¨æ ¼æ•°æ®...")
                        df, file_type = parse_table_file(file_path)
                        
                        if df is not None:
                            return df, file_type, file_path
                    
                    return None
                else:
                    # æŠ“å–é¡µé¢æ•°æ®
                    html_content, data_type = await scrape_page_data(
                        page,
                        data_description,
                        page_wait_strategy=page_wait_strategy,
                        page_timeout=page_timeout,
                        extra_wait=wait_time
                    )
                    
                    if html_content:
                        st.info("ğŸ“Š æ­£åœ¨è§£æé¡µé¢æ•°æ®...")
                        df = parse_html_to_dataframe(html_content, data_type)
                        
                        if df is not None and not df.empty:
                            return df, "page_data", url
                        else:
                            st.error("âŒ æœªèƒ½ä»é¡µé¢ä¸­æå–åˆ°æœ‰æ•ˆæ•°æ®")
                            return None
                    else:
                        st.error("âŒ æœªèƒ½è·å–é¡µé¢å†…å®¹")
                        return None
                
            except Exception as e:
                st.error(f"âŒ æŠ“å–è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
                return None
            finally:
                await browser.close()

# ä¸»ç•Œé¢
if st.button("ğŸš€ å¼€å§‹æŠ“å–", type="primary"):
    if not url:
        st.warning("âš ï¸ è¯·è¾“å…¥ç½‘é¡µ URL")
    elif scrape_mode == "ç‚¹å‡»å¯¼å‡ºæŒ‰é’®" and not button_description:
        st.warning("âš ï¸ è¯·è¾“å…¥å¯¼å‡ºæŒ‰é’®æè¿°")
    elif scrape_mode == "æŠ“å–é¡µé¢æ•°æ®" and not data_description:
        st.warning("âš ï¸ è¯·è¾“å…¥æ•°æ®æè¿°")
    elif need_login and not login_url and manual_login:
        st.warning("âš ï¸ å¦‚æœä½¿ç”¨æ‰‹åŠ¨ç™»å½•ï¼Œè¯·è¾“å…¥ç™»å½•é¡µé¢ URL")
    else:
        with st.spinner("æ­£åœ¨æŠ“å–è¡¨æ ¼æ•°æ®..."):
            result = asyncio.run(
                scrape_table(
                    url, 
                    button_description if scrape_mode == "ç‚¹å‡»å¯¼å‡ºæŒ‰é’®" else data_description, 
                    wait_time, 
                    download_timeout, 
                    headless,
                    need_login,
                    login_url,
                    use_storage if need_login else False,
                    manual_login if need_login else False,
                    scrape_mode,
                    page_wait_strategy,
                    page_timeout
                )
            )
            
            if result and result[0] is not None:
                df, file_type, file_path = result
                
                st.success(f"âœ… æˆåŠŸæŠ“å– {file_type} è¡¨æ ¼æ•°æ®ï¼")
                
                # æ˜¾ç¤ºè¡¨æ ¼ä¿¡æ¯
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("è¡Œæ•°", len(df))
                with col2:
                    st.metric("åˆ—æ•°", len(df.columns))
                with col3:
                    st.metric("æ–‡ä»¶ç±»å‹", file_type)
                
                # æ˜¾ç¤ºè¡¨æ ¼
                st.subheader("ğŸ“‹ è¡¨æ ¼æ•°æ®é¢„è§ˆ")
                # è§„é¿ Arrow ç±»å‹æ¨æ–­æŠ¥é”™ï¼Œå±•ç¤ºå‰è½¬ä¸ºå­—ç¬¦ä¸²å‰¯æœ¬
                df_display = df.copy()
                for col in df_display.columns:
                    df_display[col] = df_display[col].astype(str)
                st.dataframe(df_display, width="stretch")
                
                # ä¸‹è½½é€‰é¡¹
                st.subheader("ğŸ’¾ ä¸‹è½½æ•°æ®")
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    csv = df.to_csv(index=False).encode('utf-8')
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è½½ CSV",
                        data=csv,
                        file_name="exported_table.csv",
                        mime="text/csv"
                    )
                
                with col2:
                    import io
                    try:
                        excel_buffer = io.BytesIO()
                        with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
                            df.to_excel(writer, index=False)
                        excel_data = excel_buffer.getvalue()
                        st.download_button(
                            label="ğŸ“¥ ä¸‹è½½ Excel",
                            data=excel_data,
                            file_name="exported_table.xlsx",
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                        )
                    except ModuleNotFoundError:
                        st.warning("âš ï¸ æœªå®‰è£… openpyxlï¼Œæ— æ³•ç”Ÿæˆ Excelã€‚è¯·è¿è¡Œï¼špip install openpyxl")
                
                with col3:
                    json_data = df.to_json(orient='records', force_ascii=False, indent=2)
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è½½ JSON",
                        data=json_data.encode('utf-8'),
                        file_name="exported_table.json",
                        mime="application/json"
                    )
                
                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                with st.expander("ğŸ“Š æ•°æ®ç»Ÿè®¡"):
                    st.write(df.describe())
                
                # æ˜¾ç¤ºåˆ—ä¿¡æ¯
                with st.expander("ğŸ“ åˆ—ä¿¡æ¯"):
                    st.write(df.dtypes)
            else:
                st.error("âŒ æœªèƒ½æˆåŠŸæŠ“å–è¡¨æ ¼æ•°æ®")

# ä½¿ç”¨è¯´æ˜
with st.expander("ğŸ“– ä½¿ç”¨è¯´æ˜"):
    st.markdown("""
    ### åŠŸèƒ½è¯´æ˜
    
    è¿™ä¸ªå·¥å…·å¯ä»¥è‡ªåŠ¨ï¼š
    1. **è®¿é—®ç½‘é¡µ**ï¼šä½¿ç”¨æµè§ˆå™¨æ‰“å¼€æŒ‡å®šç½‘é¡µ
    2. **æŸ¥æ‰¾æŒ‰é’®**ï¼šæ ¹æ®æè¿°è‡ªåŠ¨æ‰¾åˆ°å¯¼å‡ºæŒ‰é’®
    3. **ç‚¹å‡»æŒ‰é’®**ï¼šè‡ªåŠ¨ç‚¹å‡»å¯¼å‡ºæŒ‰é’®
    4. **ä¸‹è½½æ–‡ä»¶**ï¼šç­‰å¾…å¹¶ä¸‹è½½å¯¼å‡ºçš„æ–‡ä»¶
    5. **è§£æè¡¨æ ¼**ï¼šè‡ªåŠ¨è§£æ CSVã€Excelã€JSON ç­‰æ ¼å¼
    6. **æ˜¾ç¤ºæ•°æ®**ï¼šåœ¨ç•Œé¢ä¸­å±•ç¤ºè¡¨æ ¼æ•°æ®
    
    ### ä½¿ç”¨æ­¥éª¤
    
    1. **è¾“å…¥ç½‘é¡µ URL**ï¼šè¦æŠ“å–çš„ç½‘é¡µåœ°å€
    2. **è¾“å…¥æŒ‰é’®æè¿°**ï¼šå¯¼å‡ºæŒ‰é’®çš„æ–‡å­—æˆ–ç‰¹å¾
       - ä¾‹å¦‚ï¼š"å¯¼å‡ºè¡¨æ ¼"ã€"Export"ã€"ä¸‹è½½ CSV" ç­‰
    3. **é…ç½®é€‰é¡¹**ï¼ˆå¯é€‰ï¼‰ï¼š
       - ç‚¹å‡»åç­‰å¾…æ—¶é—´ï¼šæŒ‰é’®ç‚¹å‡»åç­‰å¾…çš„æ—¶é—´
       - ä¸‹è½½è¶…æ—¶æ—¶é—´ï¼šç­‰å¾…æ–‡ä»¶ä¸‹è½½çš„æœ€å¤§æ—¶é—´
       - æ— å¤´æ¨¡å¼ï¼šæ˜¯å¦æ˜¾ç¤ºæµè§ˆå™¨çª—å£
    4. **ç‚¹å‡»"å¼€å§‹æŠ“å–"**ï¼šå¼€å§‹è‡ªåŠ¨æŠ“å–è¿‡ç¨‹
    
    ### æ”¯æŒçš„æ ¼å¼
    
    - âœ… CSV (.csv)
    - âœ… Excel (.xlsx, .xls)
    - âœ… JSON (.json)
    - âœ… TSV (.tsv)
    
    ### ç™»å½•åŠŸèƒ½
    
    **æ”¯æŒéœ€è¦ç™»å½•çš„ç½‘é¡µï¼š**
    
    1. **æ‰‹åŠ¨ç™»å½•ï¼ˆæ¨èï¼‰**ï¼š
       - å‹¾é€‰"éœ€è¦ç™»å½•"
       - å‹¾é€‰"æ‰‹åŠ¨ç™»å½•"
       - å…³é—­"æ— å¤´æ¨¡å¼"ï¼ˆå¯ä»¥çœ‹åˆ°æµè§ˆå™¨çª—å£ï¼‰
       - åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨è¾“å…¥ç”¨æˆ·åå’Œå¯†ç 
       - ç™»å½•æˆåŠŸåç‚¹å‡»"æˆ‘å·²ç™»å½•ï¼Œç»§ç»­"
       - å‹¾é€‰"ä¿å­˜ç™»å½•çŠ¶æ€"å¯ä»¥ä¿å­˜ç™»å½•ä¿¡æ¯ï¼Œä¸‹æ¬¡è‡ªåŠ¨ä½¿ç”¨
    
    2. **è‡ªåŠ¨ç™»å½•**ï¼š
       - å‹¾é€‰"éœ€è¦ç™»å½•"
       - è¾“å…¥ç™»å½•é¡µé¢ URLï¼ˆå¦‚æœä¸ç›®æ ‡é¡µé¢ä¸åŒï¼‰
       - å·¥å…·ä¼šå°è¯•è‡ªåŠ¨ç™»å½•ï¼ˆéœ€è¦ç½‘ç«™æ”¯æŒï¼‰
    
    **ç™»å½•çŠ¶æ€ä¿å­˜ï¼š**
    - ç™»å½•çŠ¶æ€ä¼šä¿å­˜åœ¨ `login_state.json` æ–‡ä»¶ä¸­
    - ä¸‹æ¬¡ä½¿ç”¨æ—¶å¦‚æœæ£€æµ‹åˆ°ä¿å­˜çš„çŠ¶æ€ï¼Œä¼šè‡ªåŠ¨ä½¿ç”¨
    - å¦‚æœç™»å½•å¤±æ•ˆï¼Œåˆ é™¤ `login_state.json` æ–‡ä»¶é‡æ–°ç™»å½•
    
    ### æ³¨æ„äº‹é¡¹
    
    - ç¡®ä¿ç½‘é¡µå¯ä»¥æ­£å¸¸è®¿é—®
    - å¯¼å‡ºæŒ‰é’®éœ€è¦å¯è§ä¸”å¯ç‚¹å‡»
    - **éœ€è¦ç™»å½•çš„ç½‘ç«™**ï¼šä½¿ç”¨æ‰‹åŠ¨ç™»å½•æ¨¡å¼æ›´å¯é 
    - å¦‚æœæŒ‰é’®æ˜¯åŠ¨æ€åŠ è½½çš„ï¼Œå¯èƒ½éœ€è¦å¢åŠ ç­‰å¾…æ—¶é—´
    - ç™»å½•çŠ¶æ€æ–‡ä»¶åŒ…å«æ•æ„Ÿä¿¡æ¯ï¼Œè¯·å¦¥å–„ä¿ç®¡
    """)

